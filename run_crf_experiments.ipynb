{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1l2mLcyZvaJcHBAjMgSppMqYTEJ8N52Si",
      "authorship_tag": "ABX9TyNWjeDrvwF0tKU5oc5myiQg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diogosantanaime/rec_proc/blob/main/run_crf_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U 'scikit-learn<0.24'\n",
        "! pip install sklearn_crfsuite\n",
        "! pip install seqeval"
      ],
      "metadata": {
        "id": "D7rq2sQveOoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy-J4tcCTRQb",
        "outputId": "e4a678bd-c514-4c89-d23f-f4cf27c67986"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import os\n",
        "import pickle\n",
        "import sklearn\n",
        "import scipy.stats\n",
        "import sklearn_crfsuite\n",
        "\n",
        "from nltk import pos_tag, RegexpParser\n",
        "from nltk.tokenize import word_tokenize\n",
        "from itertools import chain\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics\n",
        "from seqeval.metrics import classification_report\n",
        "from sklearn_crfsuite import CRF\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_dataset_files(dataset_files):\n",
        "    datasets = {}\n",
        "\n",
        "    for name, filepath in dataset_files.items():\n",
        "        with open(filepath, 'r') as file:\n",
        "            datasets[name] = file.read()\n",
        "\n",
        "    return datasets\n",
        "\n",
        "# Dicionário contendo os nomes dos conjuntos de dados e seus respectivos caminhos de arquivo\n",
        "dataset_files = {\n",
        "    'train_conll03': '/content/drive/MyDrive/dataset/train_conll03.txt',\n",
        "    'test_conll03': '/content/drive/MyDrive/dataset/test_conll03.txt',\n",
        "    'val_conll03': '/content/drive/MyDrive/dataset/val_conll03.txt',\n",
        "    'train_wikineural': '/content/drive/MyDrive/dataset/train_wikineural.txt',\n",
        "    'test_wikineural': '/content/drive/MyDrive/dataset/test_wikineural.txt',\n",
        "    'val_wikineural': '/content/drive/MyDrive/dataset/val_wikineural.txt',\n",
        "    'train_pet': '/content/drive/MyDrive/dataset/train_pet.txt',\n",
        "    'test_pet': '/content/drive/MyDrive/dataset/test_pet.txt',\n",
        "    'val_pet': '/content/drive/MyDrive/dataset/val_pet.txt'\n",
        "}\n",
        "\n",
        "# Chama a função para ler os arquivos do conjunto de dados\n",
        "datasets = read_dataset_files(dataset_files)\n"
      ],
      "metadata": {
        "id": "hLFn8sfA8h4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para processar os conjuntos de dados\n",
        "def preprocess_datasets(datasets):\n",
        "    processed_datasets = {}\n",
        "\n",
        "    for dataset_name, dataset_content in datasets.items():\n",
        "        # Substitui espaços em branco por tabulação\n",
        "        modified_content = dataset_content.replace(' ', '\\t')\n",
        "\n",
        "        # Remove espaços em branco extras e divide em sentenças\n",
        "        sentences = modified_content.strip().split('\\n\\n')\n",
        "\n",
        "        processed_datasets[dataset_name] = sentences\n",
        "\n",
        "    return processed_datasets\n",
        "\n",
        "# Chama a função para processar os conjuntos de dados\n",
        "datasets = preprocess_datasets(datasets)"
      ],
      "metadata": {
        "id": "vih1FcwhlnoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para remontar os conjunto de dados no formato necessário para o CRF Suite\n",
        "def remount_data(datasets):\n",
        "    remounted_datasets = {}\n",
        "    for dataset, sentences in datasets.items():\n",
        "        remounted_data = []\n",
        "        for sentence in sentences:\n",
        "            remounted_sentence = []\n",
        "            lines = sentence.strip().split('\\n')\n",
        "            for line in lines:\n",
        "                word, ner_tag = line.split('\\t')\n",
        "                tokens = word_tokenize(word)\n",
        "                pos_tags = [tag for _, tag in pos_tag(tokens)]\n",
        "                for i in range(len(tokens)):\n",
        "                    remounted_sentence.append((tokens[i], pos_tags[i], ner_tag))\n",
        "            remounted_data.append(remounted_sentence)\n",
        "        remounted_datasets[dataset] = remounted_data\n",
        "    return remounted_datasets\n",
        "\n",
        "# Chama a função para remontar os datasets no formato crf suite\n",
        "datasets = remount_data(datasets)"
      ],
      "metadata": {
        "id": "6roOw0p9K84A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word2features(sent, i):\n",
        "    \"\"\"\n",
        "    Gera as features para uma palavra específica em uma sentença.\n",
        "\n",
        "    Argumentos:\n",
        "        sent (list): A sentença contendo palavras e suas tags.\n",
        "        i (int): O índice da palavra na sentença.\n",
        "\n",
        "    Retorna:\n",
        "        dict: Um dicionário contendo as features da palavra.\n",
        "    \"\"\"\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "\n",
        "    features = {\n",
        "        'bias': 1.0,\n",
        "        'word.lower()': word.lower(),\n",
        "        'word[-3:]': word[-3:],\n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "        'postag': postag,\n",
        "        'postag[:2]': postag[:2],\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        postag1 = sent[i-1][1]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "            '-1:postag': postag1,\n",
        "            '-1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        postag1 = sent[i+1][1]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "            '+1:postag': postag1,\n",
        "            '+1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    \"\"\"\n",
        "    Gera as features para todas as palavras em uma sentença.\n",
        "\n",
        "    Argumentos:\n",
        "        sent (list): A sentença contendo palavras e suas tags.\n",
        "\n",
        "    Retorna:\n",
        "        list: Uma lista de dicionários contendo as features de cada palavra.\n",
        "    \"\"\"\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "\n",
        "def sent2labels(sent):\n",
        "    \"\"\"\n",
        "    Extrai as labels de uma sentença.\n",
        "\n",
        "    Argumentos:\n",
        "        sent (list): A sentença contendo palavras e suas tags.\n",
        "\n",
        "    Retorna:\n",
        "        list: Uma lista contendo apenas as labels da sentença.\n",
        "    \"\"\"\n",
        "    return [label for token, postag, label in sent]\n",
        "\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    \"\"\"\n",
        "    Extrai as palavras de uma sentença.\n",
        "\n",
        "    Argumentos:\n",
        "        sent (list): A sentença contendo palavras e suas tags.\n",
        "\n",
        "    Retorna:\n",
        "        list: Uma lista contendo apenas as palavras da sentença.\n",
        "    \"\"\"\n",
        "    return [token for token, postag, label in sent]\n",
        "\n",
        "\n",
        "def sent2features_to_datasets(datasets):\n",
        "    \"\"\"\n",
        "    Converte os conjuntos de dados para o formato necessário para o CRF Suite.\n",
        "\n",
        "    Argumentos:\n",
        "        datasets (list): Lista contendo os conjuntos de dados.\n",
        "\n",
        "    Retorna:\n",
        "        tuple: Tupla contendo as features (X) e as labels (y) para cada conjunto de dados.\n",
        "    \"\"\"\n",
        "    X = [sent2features(s) for s in datasets]\n",
        "    y = [sent2labels(s) for s in datasets]\n",
        "    return X, y\n"
      ],
      "metadata": {
        "id": "-oCkDUnvM06A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " crf = CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=100,\n",
        "              all_possible_transitions=True)"
      ],
      "metadata": {
        "id": "TNboPKRrz-KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertendo os conjuntos de dados para o formato necessário para o CRF Suite\n",
        "X_train, y_train = sent2features_to_datasets(datasets['train_conll03'])\n",
        "X_test, y_test = sent2features_to_datasets(datasets['test_conll03'])\n",
        "\n",
        "# Ajustar o modelo CRF aos dados de treinamento\n",
        "crf.fit(X_train, y_train)\n",
        "\n",
        "# Realizar a predição do modelo nos dados de teste\n",
        "y_pred = crf.predict(X_test)\n",
        "\n",
        "# Obtendo a lista de classes do modelo CRF\n",
        "labels = list(crf.classes_)\n",
        "\n",
        "# Removendo a classe 'O' da lista de classes\n",
        "labels.remove('O')\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kb2le_mzatn",
        "outputId": "1a735c8b-b55e-4a18-db91-5ea087a90f65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.86      0.84      0.85      1724\n",
            "        MISC       0.79      0.74      0.76       708\n",
            "         ORG       0.78      0.68      0.72      1684\n",
            "         PER       0.83      0.85      0.84      1656\n",
            "\n",
            "   micro avg       0.82      0.78      0.80      5772\n",
            "   macro avg       0.81      0.78      0.79      5772\n",
            "weighted avg       0.82      0.78      0.80      5772\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertendo os conjuntos de dados para o formato necessário para o CRF Suite\n",
        "X_train, y_train = sent2features_to_datasets(datasets['train_wikineural'])\n",
        "X_test, y_test = sent2features_to_datasets(datasets['test_wikineural'])\n",
        "\n",
        "# Ajustar o modelo CRF aos dados de treinamento\n",
        "crf.fit(X_train, y_train)\n",
        "\n",
        "# Realizar a predição do modelo nos dados de teste\n",
        "y_pred = crf.predict(X_test)\n",
        "\n",
        "# Obtendo a lista de classes do modelo CRF\n",
        "labels = list(crf.classes_)\n",
        "\n",
        "# Removendo a classe 'O' da lista de classes\n",
        "labels.remove('O')\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwKjqmhx-LTp",
        "outputId": "d83067de-e20f-489a-820b-429eab94bcd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.75      0.84      0.79      1781\n",
            "        MISC       0.76      0.70      0.72      1538\n",
            "         ORG       0.80      0.61      0.69      1103\n",
            "         PER       0.83      0.81      0.82      1601\n",
            "\n",
            "   micro avg       0.78      0.75      0.77      6023\n",
            "   macro avg       0.78      0.74      0.76      6023\n",
            "weighted avg       0.78      0.75      0.76      6023\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertendo os conjuntos de dados para o formato necessário para o CRF Suite\n",
        "X_train, y_train = sent2features_to_datasets(datasets['train_pet'])\n",
        "X_test, y_test = sent2features_to_datasets(datasets['test_pet'])\n",
        "\n",
        "# Ajustar o modelo CRF aos dados de treinamento\n",
        "crf.fit(X_train, y_train)\n",
        "\n",
        "# Realizar a predição do modelo nos dados de teste\n",
        "y_pred = crf.predict(X_test)\n",
        "\n",
        "# Obtendo a lista de classes do modelo CRF\n",
        "labels = list(crf.classes_)\n",
        "\n",
        "# Removendo a classe 'O' da lista de classes\n",
        "labels.remove('O')\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIaFKUJLArYj",
        "outputId": "1d52b225-d593-4eea-90fb-0ec0a7aee745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        precision    recall  f1-score   support\n",
            "\n",
            "            ANDGateway       0.00      0.00      0.00         2\n",
            "              Activity       0.06      0.02      0.03        60\n",
            "          ActivityData       0.05      0.02      0.02        61\n",
            "                 Actor       0.05      0.02      0.03        51\n",
            "ConditionSpecification       0.00      0.00      0.00         7\n",
            "  FurtherSpecification       0.00      0.00      0.00         4\n",
            "            XORGateway       0.00      0.00      0.00        10\n",
            "\n",
            "             micro avg       0.04      0.02      0.02       195\n",
            "             macro avg       0.02      0.01      0.01       195\n",
            "          weighted avg       0.04      0.02      0.02       195\n",
            "\n"
          ]
        }
      ]
    }
  ]
}