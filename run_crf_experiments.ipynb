{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1l2mLcyZvaJcHBAjMgSppMqYTEJ8N52Si",
      "authorship_tag": "ABX9TyPce86+G1XWv4fnYo7+ZlDj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diogosantanaime/rec_proc/blob/main/run_crf_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U 'scikit-learn<0.24'\n",
        "! pip install sklearn_crfsuite\n",
        "! pip install seqeval"
      ],
      "metadata": {
        "id": "D7rq2sQveOoQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32ffaaed-6619-43cd-c7af-cd89f7bdc760"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn<0.24\n",
            "  Downloading scikit-learn-0.23.2.tar.gz (7.2 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/7.2 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m6.5/7.2 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.1/7.2 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "Collecting sklearn_crfsuite\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn_crfsuite)\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (0.8.10)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (4.65.0)\n",
            "Installing collected packages: python-crfsuite, sklearn_crfsuite\n",
            "Successfully installed python-crfsuite-0.9.9 sklearn_crfsuite-0.3.6\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16165 sha256=b7206ea5a9606544753ab354365e30922c5f099de8ba0b8619757a04d5e0e3dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy-J4tcCTRQb",
        "outputId": "0fc4b9b0-47a9-4ed1-a5e6-b6d879858feb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import os\n",
        "import pickle\n",
        "import sklearn\n",
        "import scipy.stats\n",
        "import sklearn_crfsuite\n",
        "\n",
        "from nltk import pos_tag, RegexpParser\n",
        "from nltk.tokenize import word_tokenize\n",
        "from itertools import chain\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics\n",
        "from seqeval.metrics import classification_report, f1_score, accuracy_score\n",
        "from sklearn_crfsuite import CRF\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_dataset_files(dataset_files):\n",
        "    datasets = {}\n",
        "\n",
        "    for name, filepath in dataset_files.items():\n",
        "        with open(filepath, 'r') as file:\n",
        "            datasets[name] = file.read()\n",
        "\n",
        "    return datasets\n",
        "\n",
        "# Dicionário contendo os nomes dos conjuntos de dados e seus respectivos caminhos de arquivo\n",
        "dataset_files = {\n",
        "    'train_conll03': '/content/drive/MyDrive/dataset/train_conll03.txt',\n",
        "    'test_conll03': '/content/drive/MyDrive/dataset/test_conll03.txt',\n",
        "    'val_conll03': '/content/drive/MyDrive/dataset/val_conll03.txt',\n",
        "    'train_wikineural': '/content/drive/MyDrive/dataset/train_wikineural.txt',\n",
        "    'test_wikineural': '/content/drive/MyDrive/dataset/test_wikineural.txt',\n",
        "    'val_wikineural': '/content/drive/MyDrive/dataset/val_wikineural.txt',\n",
        "    'train_pet': '/content/drive/MyDrive/dataset/train_pet.txt',\n",
        "    'test_pet': '/content/drive/MyDrive/dataset/test_pet.txt',\n",
        "    'val_pet': '/content/drive/MyDrive/dataset/val_pet.txt'\n",
        "}\n",
        "\n",
        "# Chama a função para ler os arquivos do conjunto de dados\n",
        "datasets = read_dataset_files(dataset_files)\n"
      ],
      "metadata": {
        "id": "hLFn8sfA8h4n"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para processar os conjuntos de dados\n",
        "def preprocess_datasets(datasets):\n",
        "    processed_datasets = {}\n",
        "\n",
        "    for dataset_name, dataset_content in datasets.items():\n",
        "        # Substitui espaços em branco por tabulação\n",
        "        modified_content = dataset_content.replace(' ', '\\t')\n",
        "\n",
        "        # Remove espaços em branco extras e divide em sentenças\n",
        "        sentences = modified_content.strip().split('\\n\\n')\n",
        "\n",
        "        processed_datasets[dataset_name] = sentences\n",
        "\n",
        "    return processed_datasets\n",
        "\n",
        "# Chama a função para processar os conjuntos de dados\n",
        "datasets = preprocess_datasets(datasets)"
      ],
      "metadata": {
        "id": "vih1FcwhlnoJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para remontar os conjunto de dados no formato necessário para o CRF Suite\n",
        "def remount_data(datasets):\n",
        "    remounted_datasets = {}\n",
        "    for dataset, sentences in datasets.items():\n",
        "        remounted_data = []\n",
        "        for sentence in sentences:\n",
        "            remounted_sentence = []\n",
        "            lines = sentence.strip().split('\\n')\n",
        "            for line in lines:\n",
        "                word, ner_tag = line.split('\\t')\n",
        "                tokens = word_tokenize(word)\n",
        "                pos_tags = [tag for _, tag in pos_tag(tokens)]\n",
        "                for i in range(len(tokens)):\n",
        "                    remounted_sentence.append((tokens[i], pos_tags[i], ner_tag))\n",
        "            remounted_data.append(remounted_sentence)\n",
        "        remounted_datasets[dataset] = remounted_data\n",
        "    return remounted_datasets\n",
        "\n",
        "# Chama a função para remontar os datasets no formato crf suite\n",
        "datasets = remount_data(datasets)"
      ],
      "metadata": {
        "id": "6roOw0p9K84A"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word2features(sent, i):\n",
        "    \"\"\"\n",
        "    Gera as features para uma palavra específica em uma sentença.\n",
        "\n",
        "    Argumentos:\n",
        "        sent (list): A sentença contendo palavras e suas tags.\n",
        "        i (int): O índice da palavra na sentença.\n",
        "\n",
        "    Retorna:\n",
        "        dict: Um dicionário contendo as features da palavra.\n",
        "    \"\"\"\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "\n",
        "    features = {\n",
        "        'bias': 1.0,\n",
        "        'word.lower()': word.lower(),\n",
        "        'word[-3:]': word[-3:],\n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "        'postag': postag,\n",
        "        'postag[:2]': postag[:2],\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        postag1 = sent[i-1][1]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "            '-1:postag': postag1,\n",
        "            '-1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        postag1 = sent[i+1][1]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "            '+1:postag': postag1,\n",
        "            '+1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    \"\"\"\n",
        "    Gera as features para todas as palavras em uma sentença.\n",
        "\n",
        "    Argumentos:\n",
        "        sent (list): A sentença contendo palavras e suas tags.\n",
        "\n",
        "    Retorna:\n",
        "        list: Uma lista de dicionários contendo as features de cada palavra.\n",
        "    \"\"\"\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "\n",
        "def sent2labels(sent):\n",
        "    \"\"\"\n",
        "    Extrai as labels de uma sentença.\n",
        "\n",
        "    Argumentos:\n",
        "        sent (list): A sentença contendo palavras e suas tags.\n",
        "\n",
        "    Retorna:\n",
        "        list: Uma lista contendo apenas as labels da sentença.\n",
        "    \"\"\"\n",
        "    return [label for token, postag, label in sent]\n",
        "\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    \"\"\"\n",
        "    Extrai as palavras de uma sentença.\n",
        "\n",
        "    Argumentos:\n",
        "        sent (list): A sentença contendo palavras e suas tags.\n",
        "\n",
        "    Retorna:\n",
        "        list: Uma lista contendo apenas as palavras da sentença.\n",
        "    \"\"\"\n",
        "    return [token for token, postag, label in sent]\n",
        "\n",
        "\n",
        "def sent2features_to_datasets(datasets):\n",
        "    \"\"\"\n",
        "    Converte os conjuntos de dados para o formato necessário para o CRF Suite.\n",
        "\n",
        "    Argumentos:\n",
        "        datasets (list): Lista contendo os conjuntos de dados.\n",
        "\n",
        "    Retorna:\n",
        "        tuple: Tupla contendo as features (X) e as labels (y) para cada conjunto de dados.\n",
        "    \"\"\"\n",
        "    X = [sent2features(s) for s in datasets]\n",
        "    y = [sent2labels(s) for s in datasets]\n",
        "    return X, y\n"
      ],
      "metadata": {
        "id": "-oCkDUnvM06A"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " crf = CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=100,\n",
        "              all_possible_transitions=True)"
      ],
      "metadata": {
        "id": "TNboPKRrz-KD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertendo os conjuntos de dados para o formato necessário para o CRF Suite\n",
        "X_train, y_train = sent2features_to_datasets(datasets['train_conll03'])\n",
        "X_test, y_test = sent2features_to_datasets(datasets['test_conll03'])\n",
        "\n",
        "# Ajustar o modelo CRF aos dados de treinamento\n",
        "crf.fit(X_train, y_train)\n",
        "\n",
        "# Realizar a predição do modelo nos dados de teste\n",
        "y_pred = crf.predict(X_test)\n",
        "\n",
        "# Obtendo a lista de classes do modelo CRF\n",
        "labels = list(crf.classes_)\n",
        "\n",
        "# Removendo a classe 'O' da lista de classes\n",
        "labels.remove('O')\n",
        "\n",
        "# Calculando a acurácia\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Calculando o F-score (micro)\n",
        "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
        "\n",
        "# Calculando o F-score (macro)\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "print(\"F-score (micro):\", f1_micro)\n",
        "print(\"F-score (macro):\", f1_macro)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kb2le_mzatn",
        "outputId": "2ae0d768-391f-4f96-d1d1-1ddad41ec691"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-score (micro): 0.7999291157185895\n",
            "F-score (macro): 0.7931177214621968\n",
            "Accuracy: 0.958152649904941\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.86      0.84      0.85      1724\n",
            "        MISC       0.79      0.74      0.76       708\n",
            "         ORG       0.78      0.68      0.72      1684\n",
            "         PER       0.83      0.85      0.84      1656\n",
            "\n",
            "   micro avg       0.82      0.78      0.80      5772\n",
            "   macro avg       0.81      0.78      0.79      5772\n",
            "weighted avg       0.82      0.78      0.80      5772\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertendo os conjuntos de dados para o formato necessário para o CRF Suite\n",
        "X_train, y_train = sent2features_to_datasets(datasets['train_wikineural'])\n",
        "X_test, y_test = sent2features_to_datasets(datasets['test_wikineural'])\n",
        "\n",
        "# Ajustar o modelo CRF aos dados de treinamento\n",
        "crf.fit(X_train, y_train)\n",
        "\n",
        "# Realizar a predição do modelo nos dados de teste\n",
        "y_pred = crf.predict(X_test)\n",
        "\n",
        "# Obtendo a lista de classes do modelo CRF\n",
        "labels = list(crf.classes_)\n",
        "\n",
        "# Removendo a classe 'O' da lista de classes\n",
        "labels.remove('O')\n",
        "\n",
        "# Calculando a acurácia\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Calculando o F-score (micro)\n",
        "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
        "\n",
        "# Calculando o F-score (macro)\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "print(\"F-score (micro):\", f1_micro)\n",
        "print(\"F-score (macro):\", f1_macro)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwKjqmhx-LTp",
        "outputId": "05b977d7-9c6d-42b1-db2e-80031a6c6048"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-score (micro): 0.7662195224837594\n",
            "F-score (macro): 0.756895501033221\n",
            "Accuracy: 0.970442074110611\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.75      0.84      0.79      1781\n",
            "        MISC       0.76      0.70      0.72      1538\n",
            "         ORG       0.80      0.61      0.69      1103\n",
            "         PER       0.83      0.81      0.82      1601\n",
            "\n",
            "   micro avg       0.78      0.75      0.77      6023\n",
            "   macro avg       0.78      0.74      0.76      6023\n",
            "weighted avg       0.78      0.75      0.76      6023\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertendo os conjuntos de dados para o formato necessário para o CRF Suite\n",
        "X_train, y_train = sent2features_to_datasets(datasets['train_pet'])\n",
        "X_test, y_test = sent2features_to_datasets(datasets['test_pet'])\n",
        "\n",
        "# Ajustar o modelo CRF aos dados de treinamento\n",
        "crf.fit(X_train, y_train)\n",
        "\n",
        "# Realizar a predição do modelo nos dados de teste\n",
        "y_pred = crf.predict(X_test)\n",
        "\n",
        "# Obtendo a lista de classes do modelo CRF\n",
        "labels = list(crf.classes_)\n",
        "\n",
        "# Removendo a classe 'O' da lista de classes\n",
        "labels.remove('O')\n",
        "\n",
        "# Calculando a acurácia\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Calculando o F-score (micro)\n",
        "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
        "\n",
        "# Calculando o F-score (macro)\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "print(\"F-score (micro):\", f1_micro)\n",
        "print(\"F-score (macro):\", f1_macro)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIaFKUJLArYj",
        "outputId": "ae1c2320-d2fd-4588-87a8-e6dcafee84e7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-score (micro): 0.02247191011235955\n",
            "F-score (macro): 0.01111557818874892\n",
            "Accuracy: 0.3568627450980392\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "            ANDGateway       0.00      0.00      0.00         2\n",
            "              Activity       0.06      0.02      0.03        60\n",
            "          ActivityData       0.05      0.02      0.02        61\n",
            "                 Actor       0.05      0.02      0.03        51\n",
            "ConditionSpecification       0.00      0.00      0.00         7\n",
            "  FurtherSpecification       0.00      0.00      0.00         4\n",
            "            XORGateway       0.00      0.00      0.00        10\n",
            "\n",
            "             micro avg       0.04      0.02      0.02       195\n",
            "             macro avg       0.02      0.01      0.01       195\n",
            "          weighted avg       0.04      0.02      0.02       195\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}