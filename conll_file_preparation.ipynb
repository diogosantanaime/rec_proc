{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importação do módulo \"dataset\" do Hugging Face e carga do conjunto de dados PET (PROCESS EXTRACTION FROM TEXT).\n",
    "\n",
    "\n",
    "Dataset: https://huggingface.co/datasets/patriziobellan/PET#token-classification-task\n",
    "\n",
    "Doc: https://pdi.fbk.eu/pet-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset pet (/home/diogo/.cache/huggingface/datasets/patriziobellan___pet/token-classification/1.0.1/38434e2af57af533c400c8975f37e43c08bb77739085a3c026a862b2efb668d2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " _______ _     _ _______       _____  _______ _______      ______  _______ _______ _______ _______ _______ _______\n",
      "    |    |_____| |______      |_____] |______    |         |     \\ |_____|    |    |_____| |______ |______    |   \n",
      "    |    |     | |______      |       |______    |         |_____/ |     |    |    |     | ______| |______    |   \n",
      "                                                                                                                  \n",
      "Discover more at: [https://pdi.fbk.eu/pet-dataset/]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d0fddaed7a4156b48c8e3c2b355b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset pet (/home/diogo/.cache/huggingface/datasets/patriziobellan___pet/relations-extraction/1.0.1/38434e2af57af533c400c8975f37e43c08bb77739085a3c026a862b2efb668d2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " _______ _     _ _______       _____  _______ _______      ______  _______ _______ _______ _______ _______ _______\n",
      "    |    |_____| |______      |_____] |______    |         |     \\ |_____|    |    |_____| |______ |______    |   \n",
      "    |    |     | |______      |       |______    |         |_____/ |     |    |    |     | ______| |______    |   \n",
      "                                                                                                                  \n",
      "Discover more at: [https://pdi.fbk.eu/pet-dataset/]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22695b5f35a04d088c716543c4ba9585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "token_classification = load_dataset(\"patriziobellan/PET\", name='token-classification')\n",
    "relations_extraction = load_dataset(\"patriziobellan/PET\", name='relations-extraction')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizar as features dos conjuntos de dados: token-classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['document name', 'sentence-ID', 'tokens', 'ner-tags'],\n",
       "        num_rows: 417\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criar um dicionário de dados a partir das features do conjunto de dados: token_classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['document name', 'sentence-ID', 'tokens', 'ner-tags'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_classification_dict = {\"document name\": [], \"sentence-ID\": [], \"tokens\": [], \"ner-tags\": []}\n",
    "\n",
    "for feature in token_classification[\"test\"]:\n",
    "    token_classification_dict[\"document name\"].append(feature[\"document name\"])\n",
    "    token_classification_dict[\"sentence-ID\"].append(feature[\"sentence-ID\"])\n",
    "    token_classification_dict[\"tokens\"].append(feature[\"tokens\"])\n",
    "    token_classification_dict[\"ner-tags\"].append(feature[\"ner-tags\"])\n",
    "\n",
    "token_classification_dict.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizar as features dos conjuntos de dados: relations_extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['document name', 'tokens', 'tokens-IDs', 'ner_tags', 'sentence-IDs', 'relations'],\n",
       "        num_rows: 45\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations_extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criar um dicionário de dados a partir das features do conjunto de dados: relations_extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['document name', 'tokens', 'tokens-IDs', 'ner_tags', 'sentence-IDs', 'relations'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations_extraction_dict= {\"document name\": [], \"tokens\": [], \"tokens-IDs\": [], \"ner_tags\": [], \"sentence-IDs\": [], \"relations\": []}\n",
    "\n",
    "for feature in relations_extraction[\"test\"]:\n",
    "    relations_extraction_dict[\"document name\"].append(feature[\"document name\"])\n",
    "    relations_extraction_dict[\"tokens\"].append(feature[\"tokens\"])\n",
    "    relations_extraction_dict[\"tokens-IDs\"].append(feature[\"tokens-IDs\"])\n",
    "    relations_extraction_dict[\"ner_tags\"].append(feature[\"ner_tags\"])\n",
    "    relations_extraction_dict[\"sentence-IDs\"].append(feature[\"sentence-IDs\"])\n",
    "    relations_extraction_dict[\"relations\"].append(feature[\"relations\"])\n",
    "   \n",
    "\n",
    "relations_extraction_dict.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criar o arquivo 'file.conll' a partir dos tokens e do ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unir a lista de tokens em uma única lista colocando ' ' entre as sentenças\n",
    "tokens = []\n",
    "for token in token_classification_dict[\"tokens\"]:\n",
    "    tokens += token + [' ']\n",
    "\n",
    "# Unir as tags de NER em uma única lista\n",
    "ner_tags = sum(relations_extraction_dict[\"ner_tags\"], [])\n",
    "\n",
    "conll = []\n",
    "i = 0\n",
    "\n",
    "# Cria uma lista atendento a especificação do CONLL\n",
    "for token in tokens:\n",
    "    if token != ' ':\n",
    "        conll.append((token, ner_tags[i]))\n",
    "        i += 1\n",
    "    else:\n",
    "        conll.append((' ', ' '))\n",
    "        i -= 1\n",
    "\n",
    "# Salvar a lista num arquivo com a extensão .conll\n",
    "with open('data/file.conll', 'w') as file:\n",
    "    for item in conll:\n",
    "        file.write('{} {}\\n'.format(item[0], item[1]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
