# -*- coding: utf-8 -*-
"""run_flair_wikineural.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ar7W8-yvSE0hd_xSmghOY-aF4ro6YrHY
"""

!pip install flair

import os
import requests
import matplotlib.pyplot as plt
import random

from flair.datasets import ColumnCorpus
from flair.data import Corpus
from flair.embeddings import WordEmbeddings, StackedEmbeddings, TokenEmbeddings, FlairEmbeddings
from typing import List
from flair.models import SequenceTagger
from flair.trainers import ModelTrainer
from flair.optim import SGDW

# URL base do diretório no GitHub
base_url = 'https://github.com/Babelscape/wikineural/raw/master/data/wikineural/en'

# Pasta de destino para salvar os arquivos
destination_folder = '.'

# Lista de arquivos para download
file_list = [
    'train.conllu',
    'test.conllu',
    'val.conllu'
]

# Cria a pasta de destino, se não existir
os.makedirs(destination_folder, exist_ok=True)

# Faz o download dos arquivos
for file_name in file_list:
    url = f'{base_url}/{file_name}'
    response = requests.get(url, allow_redirects=True)
    file_path = os.path.join(destination_folder, file_name)
    with open(file_path, 'wb') as file:
        file.write(response.content)
    print(f'{file_name} baixado com sucesso.')

# Define os caminhos dos arquivos no formato local
train_file = os.path.join(destination_folder, 'train.conllu')
test_file = os.path.join(destination_folder, 'test.conllu')
dev_file = os.path.join(destination_folder, 'val.conllu')

# Define um dicionário de mapeamento das colunas do corpus
columns = {0 : 'id', 1 : 'text', 2 : 'label'}

# Define o caminho para a pasta de dados do corpus
data_folder = '.'

# Cria um objeto Corpus usando a classe ColumnCorpus
# Passa o caminho da pasta de dados, o dicionário de colunas e os arquivos de treinamento, teste e desenvolvimento
corpus: Corpus = ColumnCorpus(data_folder, columns,
                              train_file = 'train.conllu',
                              test_file = 'test.conllu',
                              dev_file = 'val.conllu')

# Imprime o número de sentenças no conjunto de treinamento
print("Número de sentenças de treinamento:", len(corpus.train))

# Imprime o número de sentenças no conjunto de teste
print("Número de sentenças de teste:", len(corpus.test))

# Imprime o número de sentenças no conjunto de desenvolvimento
print("Número de sentenças de desenvolvimento:", len(corpus.dev))

# Define o limite de sentenças para cada conjunto
train_limit = 14000
test_limit = 3500
dev_limit = 3500

# Lista para armazenar as sentenças selecionadas
train_sentences = []
test_sentences = []
dev_sentences = []

# Contadores para o número de sentenças em cada conjunto
train_count = 0
test_count = 0
dev_count = 0

# Percorre as sentenças do conjunto de treinamento
for sentence in corpus.train:
    # Verifica se o rótulo não é "<unk>"
    if sentence.labels[0].value != '<unk>':
        # Adiciona a sentença ao conjunto de treinamento
        train_sentences.append(sentence)
        train_count += 1

        # Verifica se atingiu o limite de sentenças do conjunto de treinamento
        if train_count == train_limit:
            break

# Percorre as sentenças do conjunto de teste
for sentence in corpus.test:
    # Verifica se o rótulo não é "<unk>"
    if sentence.labels[0].value != '<unk>':
        # Adiciona a sentença ao conjunto de teste
        test_sentences.append(sentence)
        test_count += 1

        # Verifica se atingiu o limite de sentenças do conjunto de teste
        if test_count == test_limit:
            break

# Percorre as sentenças do conjunto de validação
for sentence in corpus.dev:
    # Verifica se o rótulo não é "<unk>"
    if sentence.labels[0].value != '<unk>':
        # Adiciona a sentença ao conjunto de validação
        dev_sentences.append(sentence)
        dev_count += 1

        # Verifica se atingiu o limite de sentenças do conjunto de validação
        if dev_count == dev_limit:
            break

# Embaralha os conjuntos de treinamento, teste e validação
random.shuffle(train_sentences)
random.shuffle(test_sentences)
random.shuffle(dev_sentences)

# Define os subconjuntos finais com base nos limites
train_subset = train_sentences[:train_limit]
test_subset = test_sentences[:test_limit]
dev_subset = dev_sentences[:dev_limit]

# Combine os conjuntos de dados em um único corpus
combined_corpus = Corpus(train=train_subset, dev=dev_subset, test=test_subset)

# Verifique o número de sentenças em cada conjunto do corpus combinado
print("Número de sentenças de treinamento:", len(combined_corpus.train))
print("Número de sentenças de teste:", len(combined_corpus.test))
print("Número de sentenças de desenvolvimento:", len(combined_corpus.dev))

# Obtém o dicionário de rótulos do corpus
label_dictionary = combined_corpus.make_label_dictionary('label')

# Inicializa o dicionário de contagem de rótulos
label_counts = {}

# Percorre os exemplos de treinamento e conta os rótulos
for sentence in combined_corpus.train:
    for label in sentence.get_labels('label'):
        label_counts[label.value] = label_counts.get(label.value, 0) + 1

# Verifica se o rótulo '<unk>' está presente e define sua contagem como 0 se não estiver
if '<unk>' not in label_counts:
    label_counts['<unk>'] = 0

# Extrai os rótulos e as contagens
labels = label_dictionary.get_items()
counts = [label_counts[label] for label in labels]

# Plota o gráfico de barras da distribuição dos rótulos
plt.figure(figsize=(10, 6))
plt.bar(labels, counts)
plt.xlabel('Rótulos')
plt.ylabel('Contagem')
plt.title('Distribuição dos Rótulos no Corpus')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Inicializa o dicionário de contagem de rótulos para cada conjunto
train_label_counts = {}
test_label_counts = {}
dev_label_counts = {}

# Percorre os exemplos de treinamento e conta os rótulos
for sentence in combined_corpus.train:
    for label in sentence.get_labels('label'):
        train_label_counts[label.value] = train_label_counts.get(label.value, 0) + 1

# Percorre os exemplos de teste e conta os rótulos
for sentence in combined_corpus.test:
    for label in sentence.get_labels('label'):
        test_label_counts[label.value] = test_label_counts.get(label.value, 0) + 1

# Percorre os exemplos de validação e conta os rótulos
for sentence in combined_corpus.dev:
    for label in sentence.get_labels('label'):
        dev_label_counts[label.value] = dev_label_counts.get(label.value, 0) + 1

# Verifica se o rótulo '<unk>' está presente e define sua contagem como 0 se não estiver para cada conjunto
for label_counts in [train_label_counts, test_label_counts, dev_label_counts]:
    if '<unk>' not in label_counts:
        label_counts['<unk>'] = 0

# Extrai os rótulos e as contagens para cada conjunto
train_labels = label_dictionary.get_items()
train_counts = [train_label_counts[label] for label in train_labels]

test_labels = label_dictionary.get_items()
test_counts = [test_label_counts[label] for label in test_labels]

dev_labels = label_dictionary.get_items()
dev_counts = [dev_label_counts[label] for label in dev_labels]

# Plota o gráfico de barras da distribuição dos rótulos para cada conjunto
fig, axs = plt.subplots(3, 1, figsize=(10, 18))

axs[0].bar(train_labels, train_counts)
axs[0].set_xlabel('Rótulos')
axs[0].set_ylabel('Contagem')
axs[0].set_title('Distribuição dos Rótulos - Treinamento')
axs[0].tick_params(axis='x', rotation=45)

axs[1].bar(test_labels, test_counts)
axs[1].set_xlabel('Rótulos')
axs[1].set_ylabel('Contagem')
axs[1].set_title('Distribuição dos Rótulos - Teste')
axs[1].tick_params(axis='x', rotation=45)

axs[2].bar(dev_labels, dev_counts)
axs[2].set_xlabel('Rótulos')
axs[2].set_ylabel('Contagem')
axs[2].set_title('Distribuição dos Rótulos - Validação')
axs[2].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

# Define o tipo de tag
tag_type = 'label'

# Cria um dicionário de tags usando o corpus combinado
tag_dictionary = combined_corpus.make_label_dictionary(label_type=tag_type)

# Define os tipos de embeddings desejados
embedding_types: List[TokenEmbeddings] = [
    #WordEmbeddings('glove'),
    FlairEmbeddings('news-forward'),
    FlairEmbeddings('news-backward'),
]

# Cria as embeddings empilhadas
embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)

# Acesse os tipos de tags presentes no dicionário
tag_types = tag_dictionary.get_items()

# Imprima os tipos de tags
print("Tipos de tags:")
for tag in tag_types:
    print(tag)

# Imprime a primeira sentença do conjunto de treinamento convertida em uma sequência de texto com as tags anotadas.
print('\nTrain: ', combined_corpus.train[0].to_tagged_string('label'))

# Imprime a primeira sentença do conjunto de desenvolvimento convertida em uma sequência de texto com as tags anotadas.
print('Dev: ', combined_corpus.dev[0].to_tagged_string('label'))

# Imprime a primeira sentença do conjunto de teste convertida em uma sequência de texto com as tags anotadas.
print('Test: ', combined_corpus.test[0].to_tagged_string('label'))

# Importa a classe SequenceTagger do Flair

tagger : SequenceTagger = SequenceTagger(hidden_size=256,
                                       embeddings=embeddings,
                                       tag_dictionary=tag_dictionary,
                                       tag_type=tag_type,
                                       use_crf=True)

# Cria uma instância de SequenceTagger com os seguintes parâmetros:
# - hidden_size: Especifica a dimensão do espaço oculto do modelo.
# - embeddings: Passa o objeto embeddings que contém as embeddings de tokens definidas anteriormente.
# - tag_dictionary: Passa o dicionário de tags criado a partir do corpus usando o método make_tag_dictionary().
# - tag_type: Especifica o tipo de tag que o modelo irá prever.
# - use_crf: Especifica se o modelo deve usar um Conditional Random Field (CRF) para realizar a anotação de sequência.

# Treinamento do modelo

trainer: ModelTrainer = ModelTrainer(tagger, combined_corpus)
trainer.train('resources/taggers/flair-wikineural',
              learning_rate=0.1,  # Taxa de aprendizado
              mini_batch_size=32,  # Tamanho do mini-batch
              optimizer=SGDW,  # Otimizador
              max_epochs=30)  # Número máximo de épocas