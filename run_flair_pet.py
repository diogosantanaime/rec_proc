# -*- coding: utf-8 -*-
"""run_flair_pet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1URGLu5IOSxgS1K_5uh0qHDLXetsv99N6
"""

!pip install datasets
!pip install flair

# Import the necessary modules and classes
import random
from datasets import load_dataset
from flair.datasets import ColumnCorpus
from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, FlairEmbeddings
from flair.models import SequenceTagger
from flair.trainers import ModelTrainer
from flair.data import Corpus
from typing import List
from flair.optim import SGDW

# Load the required datasets and libraries

# Load the token classification dataset from the specified repository
token_classification = load_dataset("patriziobellan/PET", name='token-classification')

# Load the relation extraction dataset from the specified repository
relations_extraction = load_dataset("patriziobellan/PET", name='relations-extraction')

# Dictionary to store token classification data
token_classification_dict = {
    "document name": [],
    "sentence-ID": [],
    "tokens": [],
    "ner-tags": []
}

# Iterating over test instances in the token classification data
for feature in token_classification["test"]:
    # Storing feature values in corresponding lists in the dictionary
    token_classification_dict["document name"].append(feature["document name"])
    token_classification_dict["sentence-ID"].append(feature["sentence-ID"])
    token_classification_dict["tokens"].append(feature["tokens"])
    token_classification_dict["ner-tags"].append(feature["ner-tags"])

# Dictionary to store relation extraction data
relations_extraction_dict = {
    "document name": [],
    "tokens": [],
    "tokens-IDs": [],
    "ner_tags": [],
    "sentence-IDs": [],
    "relations": []
}

# Iterating over test instances in the relation extraction data
for feature in relations_extraction["test"]:
    # Storing feature values in corresponding lists in the dictionary
    relations_extraction_dict["document name"].append(feature["document name"])
    relations_extraction_dict["tokens"].append(feature["tokens"])
    relations_extraction_dict["tokens-IDs"].append(feature["tokens-IDs"])
    relations_extraction_dict["ner_tags"].append(feature["ner_tags"])
    relations_extraction_dict["sentence-IDs"].append(feature["sentence-IDs"])
    relations_extraction_dict["relations"].append(feature["relations"])

# List to store sentences
sentences = []

# Iterating over token lists in the token_classification_dict dictionary
for sentence in token_classification_dict["tokens"]:
    # Concatenating tokens of each sentence and adding a space between them
    sentences += sentence + [' ']

# Combining all ner_tags lists into a single list
ner = sum(relations_extraction_dict["ner_tags"], [])

# List to store ner_tags without spaces
ner_tags = []

# Iterating over ner_tags
for tag in ner:
    # Removing whitespace from ner_tags
    tag_new = tag.replace(' ', '')
    ner_tags.append(tag_new)

# List to store sentences in CoNLL format
conll = []

# Control variable for iterating over ner_tags
i = 0

# Iterating over tokens in sentences
for token in sentences:
    if token != ' ':
        # If token is not whitespace, concatenate the token and corresponding ner_tag
        conll.append(token + ' ' + ner_tags[i])
        i += 1
    else:
        # If token is whitespace, add a whitespace to the conll list and decrement the counter i
        conll.append(' ')
        i -= 1

# List to store sentences
sentences = []
# Temporary list to construct each sentence
sentence = []

# Iterating over tokens in the conll list
for token in conll:
    if token == ' ':
        # If token is whitespace, add the constructed sentence to the list of sentences
        sentences.append(sentence)
        # Reset the temporary list to construct the next sentence
        sentence = []
    else:
        # If token is not whitespace, add the token to the constructing sentence
        sentence.append(token)

# Checking if there is still a sentence being constructed at the end of the loop
if sentence:
    # If there is, add this sentence to the list of sentences
    sentences.append(sentence)

# Shuffling the list of sentences
random.shuffle(sentences)

# Ratios for data split
train_ratio = 0.8
dev_ratio = 0.1
test_ratio = 0.1

# Total size of sentences
total_size = len(sentences)
train_size = int(train_ratio*total_size)
dev_size = int(dev_ratio*total_size)
test_size = int(test_ratio*total_size)

# Sizes of training, validation, and testing sets
train_data = sentences[:train_size]
dev_data = sentences[train_size:train_size+dev_size]
test_data = sentences[train_size+dev_size:train_size+dev_size+test_size]

train_dataset = []

# Creating the training list with tokens and newline markers
for sentence in train_data:
    train_dataset += sentence + ['\n']

# Writing the training list to the "train.txt" file
with open('train.txt', 'w') as f:
    for token in train_dataset:
        if token != '\n':
            f.write(token + '\n')
        else:
            f.write('\n')

test_dataset = []

# Creating the test list with tokens and newline markers
for sentence in test_data:
    test_dataset += sentence + ['\n']

# Writing the test list to the "test.txt" file
with open('test.txt', 'w') as f:
    for token in test_dataset:
        if token != '\n':
            f.write(token + '\n')
        else:
            f.write('\n')

dev_dataset = []

# Creating the validation list with tokens and newline markers
for sentence in dev_data:
    dev_dataset += sentence + ['\n']

# Writing the validation list to the "dev.txt" file
with open('dev.txt', 'w') as f:
    for token in dev_dataset:
        if token != '\n':
            f.write(token + '\n')
        else:
            f.write('\n')

data_folder = "."

columns = {0: "text", 1: "ner"}

corpus = ColumnCorpus(data_folder, columns,
                      train_file="train.txt",
                      test_file="test.txt",
                      dev_file="dev.txt")

# Define o tipo de tag
tag_type = 'ner'

# Cria um dicionário de tags usando o corpus combinado
tag_dictionary = corpus.make_label_dictionary(label_type=tag_type)

# Define os tipos de embeddings desejados
embedding_types: List[TokenEmbeddings] = [
    #WordEmbeddings('glove'),
    FlairEmbeddings('news-forward'),
    FlairEmbeddings('news-backward'),
    #PooledFlairEmbeddings('news-forward', pooling='min'),
    #PooledFlairEmbeddings('news-backward', pooling='min'),
]

# Cria as embeddings empilhadas
embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)

tagger : SequenceTagger = SequenceTagger(hidden_size=256,
                                       embeddings=embeddings,
                                       tag_dictionary=tag_dictionary,
                                       tag_type=tag_type,
                                       use_crf=True)

trainer: ModelTrainer = ModelTrainer(tagger, corpus)
trainer.train('resources/taggers/pet',
              learning_rate=0.1,  # Taxa de aprendizado
              mini_batch_size=32,  # Tamanho do mini-batch
              optimizer=SGDW,  # Otimizador
              max_epochs=30)  # Número máximo de épocas