# -*- coding: utf-8 -*-
"""datasets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BUPZhrEj46Oj0o6XmBfoJaeRaxGP68T2
"""

!pip install datasets

from google.colab import drive
drive.mount('/content/drive')

from datasets import load_dataset

token_classification = load_dataset(
    "patriziobellan/PET",
    name='token-classification'
    )

relations_extraction = load_dataset(
    "patriziobellan/PET", 
    name='relations-extraction'
    )

token_classification, relations_extraction

token_classification_dict = {
    "document name": [], 
    "sentence-ID": [], 
    "tokens": [], 
    "ner-tags": []
    }

for feature in token_classification["test"]:
    token_classification_dict["document name"].append(feature["document name"])
    token_classification_dict["sentence-ID"].append(feature["sentence-ID"])
    token_classification_dict["tokens"].append(feature["tokens"])
    token_classification_dict["ner-tags"].append(feature["ner-tags"])

relations_extraction_dict= {
    "document name": [],
    "tokens": [], 
    "tokens-IDs": [],
    "ner_tags": [], 
    "sentence-IDs": [], 
    "relations": []
    }

for feature in relations_extraction["test"]:
    relations_extraction_dict["document name"].append(feature["document name"])
    relations_extraction_dict["tokens"].append(feature["tokens"])
    relations_extraction_dict["tokens-IDs"].append(feature["tokens-IDs"])
    relations_extraction_dict["ner_tags"].append(feature["ner_tags"])
    relations_extraction_dict["sentence-IDs"].append(feature["sentence-IDs"])
    relations_extraction_dict["relations"].append(feature["relations"])

sentences = []

for sentence in token_classification_dict["tokens"]:
  sentences += sentence + [' ']

ner = sum(relations_extraction_dict["ner_tags"], [])

ner_tags = []

for tag in ner:
  tag_new = tag.replace(' ', '')
  ner_tags.append(tag_new)

conll = []
i = 0

for token in sentences:
  if token != ' ':
    conll.append(token + ' ' + ner_tags[i])
    i += 1
  else:
    conll.append(' ')
    i -= 1

sentences = []
sentence = []

for token in conll:
  if token == ' ':
    sentences.append(sentence)
    sentence = []
  else:
    sentence.append(token)
if sentence:
  sentences.append(sentence)

import random
random.shuffle(sentences)

train_ratio = 0.8
dev_ratio = 0.1
test_ratio = 0.1

total_size = len(sentences)
train_size = int(train_ratio*total_size)
dev_size = int(dev_ratio*total_size)
test_size = int(test_ratio*total_size)

train_data = sentences[:train_size]
dev_data = sentences[train_size:train_size+dev_size]
test_data = sentences[train_size+dev_size:train_size+dev_size+test_size]

train_dataset = []

for sentence in train_data:
    train_dataset += sentence + ['\n']

with open('/content/drive/MyDrive/dataset/train.conll', 'w') as f:
    for token in train_dataset:
      if token != '\n':
        f.write(token + '\n')
      else:
        f.write('\n')

test_dataset = []

for sentence in test_data:
    test_dataset += sentence + ['\n']

with open('/content/drive/MyDrive/dataset/test.conll', 'w') as f:
    for token in test_dataset:
      if token != '\n':
        f.write(token + '\n')
      else:
        f.write('\n')

dev_dataset = []

for sentence in dev_data:
    dev_dataset += sentence + ['\n']

with open('/content/drive/MyDrive/dataset/dev.conll', 'w') as f:
    #f.writelines(dev_dataset)
    for token in dev_dataset:
      if token != '\n':
        f.write(token + '\n')
      else:
        f.write('\n')
